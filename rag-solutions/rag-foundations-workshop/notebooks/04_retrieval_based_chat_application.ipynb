{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a5ab2f-d044-4956-b75b-7408d9c3e323",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation with Amazon Bedrock - Enhancing Chat Applications with RAG\n",
    "\n",
    "> *PLEASE NOTE: This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*\n",
    "\n",
    "---\n",
    "\n",
    "## Chat with LLMs Overview\n",
    "\n",
    "Conversational interfaces such as chatbots and virtual assistants can be used to enhance the user experience for your customers. Chatbots can be used in a variety of applications, such as customer service, sales, and e-commerce, to provide quick and efficient responses to users.\n",
    "\n",
    "The key technical detail which we need to include in our system to enable a chat feature is conversational memory. This way, customers can ask follow up questions and the LLM will understand what the customer has already said in the past. The image below shows how this is orchestrated at a high level.\n",
    "\n",
    "![Amazon Bedrock - Conversational Interface](./images/chatbot_bedrock.png)\n",
    "\n",
    "## Extending Chat with RAG\n",
    "\n",
    "However, in our workshop's situation, we want to be able to enable a customer to ask follow up questions regarding documentation we provide through RAG. This means we need to build a system which has conversational memory AND contextual retrieval built into the text generation.\n",
    "\n",
    "![4](./images/context-aware-chatbot.png)\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf5603",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup `boto3` Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ba54e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "region = os.environ.get(\"AWS_REGION\")\n",
    "boto3_bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=region,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100c1f4a-d295-478a-b2df-487b67c0472d",
   "metadata": {},
   "source": [
    "---\n",
    "## Using LangChain for Conversation Memory\n",
    "\n",
    "We will use LangChain's `ConversationBufferMemory` class provides an easy way to capture conversational memory for LLM chat applications. Let's check out an example of Claude being able to retrieve context through conversational memory below.\n",
    "\n",
    "Similar to the last workshop, we will use both a prompt template and a LangChain LLM for this example. Note that this time our prompt template includes a `{history}` variable where our chat history will be included to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e48e507d-89be-4e1f-87b7-0fa1f7ffb187",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "CHAT_PROMPT_TEMPLATE = '''You are a helpful conversational assistant.\n",
    "{history}\n",
    "\n",
    "Human: {human_input}\n",
    "\n",
    "Assistant:\n",
    "'''\n",
    "PROMPT = PromptTemplate.from_template(CHAT_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "961b106c-baa4-474c-97f9-eca7d868e52e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_aws.llms import BedrockLLM\n",
    "\n",
    "llm = BedrockLLM(\n",
    "    client=boto3_bedrock,\n",
    "    model_id=\"anthropic.claude-instant-v1\",\n",
    "    model_kwargs={\n",
    "        \"max_tokens_to_sample\": 500,\n",
    "        \"temperature\": 0.9,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24656b5",
   "metadata": {},
   "source": [
    "The `ConversationBufferMemory` class is instantiated here and you will notice that we use Claude specific human and assistant prefixes. When we initialize the memory, the history is blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "23e84d14-9b71-4235-83e3-045b87e6cbcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(human_prefix=\"\\nHuman\", ai_prefix=\"\\nAssistant\")\n",
    "history = memory.load_memory_variables({})['history']\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217109c8",
   "metadata": {},
   "source": [
    "We now ask Claude a simple question \"How can I check for imbalances in my model?\". The LLM responds to the question and we can use the `add_user_message` and `add_ai_message` functions to save the input and output into memory. We can then retrieve the entire conversation history and print the response. Currently the model will still return answer using the data it was trained upon. Further will examine how to get a curated answer using our own FAq's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "38515d05-6437-43fa-865e-1fa80b6c3fed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Human: How can I check for imbalances in my model?\n",
       "\n",
       "Assistant: Here are a few things you can do to check for imbalances in your machine learning model:\n",
       "\n",
       "- Compute accuracy and other metrics on different subgroups of your data (e.g. by gender, age, location, etc.). Look for large discrepancies that indicate the model may be underperforming for certain subgroups.\n",
       "\n",
       "- Perform stratified validation and testing. Split your data into folds or test/train sets separately for each subgroup, rather than mixing all data together. This helps catch when the model fits some subgroups better than others. \n",
       "\n",
       "- Analyze feature importances and weights. See if features correlated with sensitive attributes like gender or ethnicity are getting unduly high or low importance that could indicate unfair biases.\n",
       "\n",
       "- Try calibrating predictions. Compare predicted probabilities to observed outcomes within subgroups to check for calibration errors that suggest over or under confidence for some groups.\n",
       "\n",
       "- Run a fairness metric evaluation. Tools like model cards and AI fairness 360 can compute metrics like equalized odds, demographic parity, etc. to flag unjustified performance differences.\n",
       "\n",
       "- Get data labels checked. Have human evaluators label a sample of predictions to validate the model isn't systematically making unfair mistakes on some subgroups.\n",
       "\n",
       "Regularly monitoring these types of subgroup analyses and fairness metrics during model development can help ensure biases and imbalances don't creep in."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "human_input = 'How can I check for imbalances in my model?'\n",
    "\n",
    "prompt_data = PROMPT.format(human_input=human_input, history=history , context=\"\")\n",
    "ai_output = llm(prompt_data)\n",
    "\n",
    "memory.chat_memory.add_user_message(human_input)\n",
    "memory.chat_memory.add_ai_message(ai_output.strip())\n",
    "\n",
    "history = memory.load_memory_variables({})['history']\n",
    "display(Markdown(history))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4df10c",
   "metadata": {},
   "source": [
    "Now we will ask a follow up question about the kind of imbalances does it detect and save the input and outputs again. Notice how the model is able to understand that when the human says \"it\", because it has access to the context of the chat history, the model is able to accurately understand what the user is asking about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c2b6b6f1-7101-4603-87a8-9b6b0032226d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Here are some common types of imbalances that model checking aims to detect:\n",
       "\n",
       "- Accuracy imbalances - When the model's predictive performance varies significantly across subgroups, with much higher or lower accuracy for some groups.\n",
       "\n",
       "- Calibration imbalances - If the model is over or under confident in its predictions for certain subgroups, indicated by calibration errors between predicted probabilities and observed outcomes. \n",
       "\n",
       "- Feature importance imbalances - When features correlated with sensitive attributes like gender receive unduly high or low importance scores, which could reflect unfair biases.\n",
       "\n",
       "- Demographic parity imbalances - If the model's predictions differ substantially across protected subgroups, even when controlling for legitimate factors. This detects disparate treatment.\n",
       "\n",
       "- Equalized odds imbalances - When false positive or false negative rates vary across subgroups, indicating the model may not be equally beneficial or harmful to all groups. \n",
       "\n",
       "- Treatment effect variable imbalances - If a model's treatment effect (how well it predicts an outcome) is significantly different depending on subgroup membership. \n",
       "\n",
       "- Mistake analysis imbalances - By analyzing types of errors on subsets of the data, imbalances may appear if certain groups are systematically more likely to have certain kinds of mistakes made on their data.\n",
       "\n",
       "The goal is to detect any unjustified performance discrepancies or treatment differences between subgroups that could arise from biases in the data or model training process. A variety of statistical tests and metrics help flag these types of potential imbalances."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "human_input = 'What kind does it detect?'\n",
    "\n",
    "prompt_data = PROMPT.format(human_input=human_input, history=history, context=\"\")\n",
    "ai_output = llm(prompt_data)\n",
    "\n",
    "memory.chat_memory.add_user_message(human_input)\n",
    "memory.chat_memory.add_ai_message(ai_output.strip())\n",
    "\n",
    "display(Markdown(ai_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b107b4f-358b-4f29-887a-15ef5341abaf",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating a class to help facilitate conversation\n",
    "\n",
    "To help create some structure around these conversations, we create a custom `Conversation` class below. This class will hold a stateful conversational memory and be the base for conversational RAG later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "396cac77-00a4-4ae2-913f-56ae1dbc60ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    def __init__(self, client, model_id: str=\"anthropic.claude-instant-v1\") -> None:\n",
    "        \"\"\"instantiates a new rag based conversation\n",
    "\n",
    "        Args:\n",
    "            model_id (str, optional): which bedrock model to use for the conversational agent. Defaults to \"anthropic.claude-instant-v1\".\n",
    "        \"\"\"\n",
    "\n",
    "        # instantiate memory\n",
    "        self.memory = ConversationBufferMemory(human_prefix=\"\\nHuman\", ai_prefix=\"\\nAssistant\")\n",
    "\n",
    "        # instantiate LLM connection\n",
    "        self.llm = BedrockLLM(\n",
    "            client=client,\n",
    "            model_id=model_id,\n",
    "            model_kwargs={\n",
    "                \"max_tokens_to_sample\": 500,\n",
    "                \"temperature\": 0.9,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def ai_respond(self, user_input: str=None):\n",
    "        \"\"\"responds to the user input in the conversation with context used\n",
    "\n",
    "        Args:\n",
    "            user_input (str, optional): user input. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            ai_output (str): response from AI chatbot\n",
    "        \"\"\"\n",
    "\n",
    "        # format the prompt with chat history and user input\n",
    "        history = self.memory.load_memory_variables({})['history']\n",
    "        llm_input = PROMPT.format(history=history, human_input=user_input)\n",
    "\n",
    "        # respond to the user with the LLM\n",
    "        ai_output = self.llm(llm_input).strip()\n",
    "\n",
    "        # store the input and output\n",
    "        self.memory.chat_memory.add_user_message(user_input)\n",
    "        self.memory.chat_memory.add_ai_message(ai_output.strip())\n",
    "\n",
    "        return ai_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a24eed",
   "metadata": {},
   "source": [
    "Let's see the class in action with two contextual questions. Again, notice the model is able to correctly interpret the context because it has memory of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "154f8e9b-6088-4a65-859b-e6087b1949c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = Conversation(client=boto3_bedrock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "63d35169-0dee-430e-9646-5d79dc713109",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are a few things you can do to check for imbalances in your model:\n",
       "\n",
       "- Check the training data for class imbalance. Look at the distribution of labels/classes in your training data. If one class is significantly more represented than others, that could lead to bias.\n",
       "\n",
       "- Evaluate model performance on each class separately. Run your model on a validation set and calculate metrics like accuracy, precision, recall separately for each class. Imbalances will show up as significantly different metrics across classes. \n",
       "\n",
       "- Look at the loss curve during training. If the loss is decreasing faster or slower for some classes compared to others, that indicates potential imbalance issues.\n",
       "\n",
       "- Visualize model predictions. Generate predictions on a test set and visualize the results, e.g. in a confusion matrix. Disproportionate errors across classes may point to imbalances.\n",
       "\n",
       "- Stratify validation/test sets. When creating validation/test sets, try to preserve the class distribution of the original data to get a true sense of model performance.\n",
       "\n",
       "- Oversample minority classes or downsample majority classes. Adding more samples of under-represented classes can help address imbalances in the training data itself.\n",
       "\n",
       "- Use loss functions robust to class imbalance. Metrics like F1 score or weighted losses can push the model to perform well on all classes, not just common ones.\n",
       "\n",
       "Regular monitoring and analysis are needed to catch any imbalances creeping into your model during development. The earlier you address them, the better."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = chat.ai_respond('How can I check for imbalances in my model?')\n",
    "display(Markdown(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "43fb63f8-d8fc-4a4e-9bc3-6ccb1c30d002",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "There's no single kind of imbalance that models can detect. Different types of potential imbalances to be aware of include:\n",
       "\n",
       "- Class imbalance - When some classes or labels are significantly under-represented compared to others in the training data. This is one of the most common types.\n",
       "\n",
       "- Sample imbalance - When certain types of samples/data points are over or under-represented, even within the same class. For example, some examples may be more difficult than others.\n",
       "\n",
       "- Attribute imbalance - When certain attributes/features are disproportionately present or absent across classes. For example, if one class is strongly correlated with a particular feature value. \n",
       "\n",
       "- Threshold imbalance - When decision thresholds used for classification are not well-calibrated for all classes. Minor classes may need their own thresholds.\n",
       "\n",
       "- Missing data imbalance - When some classes are missing attribute values more often than others in an unbalanced way.\n",
       "\n",
       "- Concept drift imbalance - When the distribution of data changes over time in an unbalanced manner, affecting some classes more than others.\n",
       "\n",
       "- Multiclass imbalance - When performing multiclass classification, some classes may be under-represented compared to others. \n",
       "\n",
       "By analyzing model performance, predictions and metrics separately for each class/group, various types of potential imbalances can be detected. The monitoring techniques mentioned in my previous answer apply generally without being specific to any single imbalance type. The goal is to identify disproportionate effects on any group."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = chat.ai_respond('What kind does it detect?')\n",
    "display(Markdown(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192a46c-5a1f-4683-8c66-c47f05477468",
   "metadata": {},
   "source": [
    "---\n",
    "## Combining RAG with Conversation\n",
    "\n",
    "Now that we have a conversational system built, lets incorporate the RAG system we built in notebook 02 into the chat paradigm. \n",
    "\n",
    "First, we will create the same vector store with LangChain and FAISS from the last notebook.\n",
    "\n",
    "Our goal is to create a curated response from the model and only use the FAQ's we have provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "23b07c75-896c-4a99-b214-ad42c12651e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# create instantiation to embedding model\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=boto3_bedrock,\n",
    "    model_id=\"amazon.titan-embed-text-v1\"\n",
    ")\n",
    "\n",
    "# create vector store\n",
    "vs = FAISS.load_local('../faiss-index/langchain/', embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecb169d",
   "metadata": {},
   "source": [
    "### Visualize Semantic Search \n",
    "\n",
    "⚠️ ⚠️ ⚠️ This section is for Advanced Practioners. Please feel free to run through these cells and come back later to re-examine the concepts ⚠️ ⚠️ ⚠️ \n",
    "\n",
    "Let's see how the semantic search works:\n",
    "1. First we calculate the embeddings vector for the query, and\n",
    "2. then we use this vector to do a similarity search on the store\n",
    "\n",
    "\n",
    "##### Citation\n",
    "We will also be able to get the `citation` or the underlying documents which our Vector Store matched to our query. This is useful for debugging and also measuring the quality of the vector stores. let us look at how the underlying Vector store calculates the matches\n",
    "\n",
    "##### Vector DB Indexes\n",
    "One of the key components of the Vector DB is to be able to retrieve documents matching the query with accuracy and speed. There are multiple algorithims for the same and some examples can be [read here](https://thedataquarry.com/posts/vector-db-3/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#- helpful function to display in tabular format\n",
    "\n",
    "def display_table(data):\n",
    "    html = \"<table>\"\n",
    "    for row in data:\n",
    "        html += \"<tr>\"\n",
    "        for field in row:\n",
    "            html += \"<td>%s</td>\"%(field)\n",
    "        html += \"</tr>\"\n",
    "    html += \"</table>\"\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4f0dd6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1474609375, 0.77734375, 0.26953125, -0.55859375, 0.0478515625, -0.435546875, -0.0576171875, -0.0003032684326171875, -0.5703125, -0.337890625]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Let us look at the documents which had the relevant information pertaining to our query"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "What kind of bias does SageMaker Clarify detect?,\" Measuring bias in ML models is a first step to mitigating bias. Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You must choose bias notions and metrics that are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor). For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups. For example, start by comparing the error rates (how likely a model's prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example).\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "--------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "How do I build an ML model to generate accurate predictions in SageMaker Canvas?,\" Once you have connected sources, selected a dataset, and prepared your data, you can select the target column that you want to predict to initiate a model creation job. SageMaker Canvas will automatically identify the problem type, generate new relevant features, test a comprehensive set of prediction models using ML techniques such as linear regression, logistic regression, deep learning, time-series forecasting, and gradient boosting, and build the model that makes accurate predictions based on your dataset.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "--------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "v = embedding_model.embed_query(\"How can I check for imbalances in my model?\")\n",
    "print(v[0:10])\n",
    "results = vs.similarity_search_by_vector(v, k=2)\n",
    "display(Markdown('Let us look at the documents which had the relevant information pertaining to our query'))\n",
    "for r in results:\n",
    "    display(Markdown(r.page_content))\n",
    "    display(Markdown('-'*20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity Search\n",
    "\n",
    "##### Distance scoring in Vector Data bases\n",
    "[Distance scores](https://weaviate.io/blog/distance-metrics-in-vector-search) are the key in vector searches. Here are some FAISS specific methods. One of them is similarity_search_with_score, which allows you to return not only the documents but also the distance score of the query to them. The returned distance score is L2 distance ( Squared Euclidean) . Therefore, a lower score is better. Further in FAISS we have similarity_search_with_score (ranked by distance: low to high) and similarity_search_with_relevance_scores ( ranked by relevance: high to low) with both using the distance strategy. The similarity_search_with_relevance_scores calculates the relevance score as 1 - score. For more details of the various distance scores [read here](https://milvus.io/docs/metric.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "##### Let us look at the documents based on EUCLIDEAN_DISTANCE which will be used to answer our question 'What kind of bias does Clarify detect ?'"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "--------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>Documents</td><td>Meta-data</td></tr><tr><td>What kind of bias does SageMaker Clarify detect?,\" Measuring bias in ML models is a first step to mitigating bias. Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You must choose bias notions and metrics that are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor). For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups. For example, start by comparing the error rates (how likely a model's prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example).\"</td><td>{}</td></tr><tr><td>How does SageMaker Clarify improve model explainability?, SageMaker Clarify is integrated with SageMaker Experiments to provide a feature importance graph detailing the importance of each input for your model’s overall decision-making process after the model has been trained. These details can help determine if a particular model input has more influence than it should on overall model behavior. SageMaker Clarify also makes explanations for individual predictions available through an API.</td><td>{}</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"##### Let us look at the documents based on {vs.distance_strategy.name} which will be used to answer our question 'What kind of bias does Clarify detect ?'\"))\n",
    "\n",
    "context = vs.similarity_search('What kind of bias does Clarify detect ?', k=2)\n",
    "#-  langchain.schema.document.Document\n",
    "display(Markdown('-'*20))\n",
    "list_context = [[doc.page_content, doc.metadata] for doc in context]\n",
    "list_context.insert(0, ['Documents', 'Meta-data'])\n",
    "display_table(list_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first look at the Page context and the meta data associated with the documents. Now let us look at the L2 scores based on the distance scoring as explained above. Lower score is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "##### Similarity Search Table with relevancy score."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "--------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>Documents</td><td>Relevancy Score</td></tr><tr><td>page_content='What kind of bias does SageMaker Clarify detect?,\" Measuring bias in ML models is a first step to mitigating bias. Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You must choose bias notions and metrics that are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor). For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups. For example, start by comparing the error rates (how likely a model's prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example).\"'</td><td>130.92531</td></tr><tr><td>page_content='How does SageMaker Clarify improve model explainability?, SageMaker Clarify is integrated with SageMaker Experiments to provide a feature importance graph detailing the importance of each input for your model’s overall decision-making process after the model has been trained. These details can help determine if a particular model input has more influence than it should on overall model behavior. SageMaker Clarify also makes explanations for individual predictions available through an API.'</td><td>188.70465</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#- relevancy of the documents\n",
    "results = vs.similarity_search_with_score(\"What kind of bias does Clarify detect ?\", k=2, fetch_k=3)\n",
    "display(Markdown('##### Similarity Search Table with relevancy score.'))\n",
    "display(Markdown('-'*20))\n",
    "results.insert(0, ['Documents', 'Relevancy Score'])\n",
    "display_table(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Marginal Relevancy score\n",
    "\n",
    "Maximal Marginal Relevance  has been introduced in the paper [The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries](https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf). Maximal Marginal Relevance tries to reduce the redundancy of results while at the same time maintaining query relevance of results for already ranked documents/phrases etc. In the below results since we have a very limited data set it might not make a difference but for larger data sets the query will theoritically run faster while still preserving the over all relevancy of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "##### Let us look at MRR scores"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>Document</td><td>MRR Score</td></tr><tr><td>page_content='What kind of bias does SageMaker Clarify detect?,\" Measuring bias in ML models is a first step to mitigating bias. Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You must choose bias notions and metrics that are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor). For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups. For example, start by comparing the error rates (how likely a model's prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example).\"'</td><td>130.92531</td></tr><tr><td>page_content='How does SageMaker Clarify improve model explainability?, SageMaker Clarify is integrated with SageMaker Experiments to provide a feature importance graph detailing the importance of each input for your model’s overall decision-making process after the model has been trained. These details can help determine if a particular model input has more influence than it should on overall model behavior. SageMaker Clarify also makes explanations for individual predictions available through an API.'</td><td>188.70465</td></tr><tr><td>page_content='What is the underlying tuning algorithm for Automatic Model Tuning?,\" Currently, the algorithm for tuning hyperparameters is a customized implementation of Bayesian Optimization. It aims to optimize a customer-specified objective metric throughout the tuning process. Specifically, it checks the object metric of completed training jobs, and uses the knowledge to infer the hyperparameter combination for the next training job.\"\n",
       "Does Automatic Model Tuning recommend specific hyperparameters for tuning?,\" No. How certain hyperparameters impact the model performance depends on various factors, and it is hard to definitively say one hyperparameter is more important than the others and thus needs to be tuned. For built-in algorithms within SageMaker, we do call out whether or not a hyperparameter is tunable.\"'</td><td>281.98914</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#- normalizing the relevancy\n",
    "display(Markdown('##### Let us look at MRR scores'))\n",
    "results = vs.max_marginal_relevance_search_with_score_by_vector(embedding_model.embed_query(\"What kind of bias does Clarify detect ?\"), k=3)\n",
    "results.insert(0, [\"Document\", \"MRR Score\"])\n",
    "display_table(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update embeddings of the Vector Databases\n",
    "\n",
    "Update of documents happens all the time and we have multiple versions of the documents. Which means we need to also factor how do we update the embeddings in our Vector Data bases. Fortunately we have and can leverage the meta data to update embeddings\n",
    "\n",
    "The key steps are:\n",
    "1. Load the new embeddings and add the meta data stating the version as 2\n",
    "2. Merge to the exisiting Vector database\n",
    "3. Run the query using the filter to only search in the new index and get the latest documents for the same query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of split docs=6\n"
     ]
    }
   ],
   "source": [
    "# create vector store\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "loader = CSVLoader(\n",
    "    file_path=\"../data/sagemaker/sm_faq_v2.csv\",\n",
    "    csv_args={\n",
    "        \"delimiter\": \",\",\n",
    "        \"quotechar\": '\"',\n",
    "        \"fieldnames\": [\"Question\", \"Answer\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "#docs_split = loader.load()\n",
    "docs_split = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separator=\",\").split_documents(loader.load())\n",
    "list_of_documents = [Document(page_content=doc.page_content, metadata=dict(page='v2')) for doc in docs_split]\n",
    "print(f\"Number of split docs={len(docs_split)}\")\n",
    "db = FAISS.from_documents(list_of_documents, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run a query against version 2 of the documents\n",
    "Let us run the query agsint our exisiting vector data base and we will see the the exisiting or the version 1 of the documents coming back. If we run with the filter since those do not exist in our vector Database we will see no results returned or an empty list back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Running the query with V2 of the document we get []:\n"
     ]
    }
   ],
   "source": [
    "# Run the query with requesting data from version 2 which does not exist\n",
    "vs = FAISS.load_local('../faiss-index/langchain/', embedding_model, allow_dangerous_deserialization=True)\n",
    "search_query = \"How can I check for imbalances in my model?\"\n",
    "#print(f\"Running with v1 of the documents we get response of {vs.similarity_search_with_score(query=search_query, k=1, fetch_k=4)}\")\n",
    "print(\"-\"*20)\n",
    "print(f\"Running the query with V2 of the document we get {vs.similarity_search_with_score(query=search_query, filter=dict(page='v2'), k=1)}:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a new version of the document\n",
    "We will create the version 2 of the documents and use meta data to add to our original index. Once done we will then apply a filter in our query which will return to us the documents newly added. Run the query now after adding version of the documents\n",
    "\n",
    "We will also examine a way to speed up our searches and queries and look at another way to narrow the search using the  fetch_k parameter when calling similarity_search with filters. Usually you would want the fetch_k to be more than the k parameter. This is because the fetch_k parameter is the number of documents that will be fetched before filtering. If you set fetch_k to a low number, you might not get enough documents to filter from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - now let us add version 2 of the data set and run query from that\n",
    "\n",
    "vs.merge_from(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query complete merged data base with no filters\n",
    "Run the query against the fully merged DB without any filters for the meta data and we see that it returns the top results of the new V2 data and also the top results of the v1 data. Essentially it will match and return data closest to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>Document</td><td>Meta-Data</td><td>Score</td></tr><tr><td>Question: How can I check for imbalances in my model?\n",
       "Answer: Amazon SageMaker Clarify Version 2 will helps improve model transparency. SageMaker Clarify checks for imbalances during data preparation, after training, and ongoing over time</td><td>{'page': 'v2'}</td><td>154.83807</td></tr><tr><td>What kind of bias does SageMaker Clarify detect?,\" Measuring bias in ML models is a first step to mitigating bias. Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You must choose bias notions and metrics that are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor). For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups. For example, start by comparing the error rates (how likely a model's prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example).\"</td><td>{}</td><td>229.07724</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# - run the query again\n",
    "search_query_v2 = \"How can I check for imbalances in my model?\"\n",
    "results_with_scores = vs.similarity_search_with_score(search_query_v2, k=2, fetch_k=3)\n",
    "results_with_scores = [[doc.page_content, doc.metadata, score] for doc, score in results_with_scores]\n",
    "results_with_scores.insert(0, ['Document', 'Meta-Data', 'Score'])\n",
    "display_table(results_with_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query with Filter\n",
    "Now we will ask to search only against the version 2 of the data and use filter criteria against it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>Document</td><td>Meta-Data</td><td>Score</td></tr><tr><td>Question: How can I check for imbalances in my model?\n",
       "Answer: Amazon SageMaker Clarify Version 2 will helps improve model transparency. SageMaker Clarify checks for imbalances during data preparation, after training, and ongoing over time</td><td>{'page': 'v2'}</td><td>154.83807</td></tr><tr><td>Question: What kind of bias does SageMaker Clarify detect?\n",
       "Answer: Measuring bias in ML models is a first step to mitigating bias.</td><td>{'page': 'v2'}</td><td>268.96292</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# - run the query again\n",
    "search_query_v2 = \"How can I check for imbalances in my model?\"\n",
    "results_with_scores = vs.similarity_search_with_score(search_query_v2, filter=dict(page='v2'), k=2, fetch_k=3)\n",
    "results_with_scores = [[doc.page_content, doc.metadata, score] for doc, score in results_with_scores]\n",
    "results_with_scores.insert(0, ['Document', 'Meta-Data', 'Score'])\n",
    "display_table(results_with_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query for new data\n",
    "Now let us ask a question which exists only on the version 2 of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>Document</td><td>Meta-Data</td><td>Score</td></tr><tr><td>Question: Can i use Quantum computing?\n",
       "Answer: Yes SageMaker version sometime in future will let you run quantum computing</td><td>{'page': 'v2'}</td><td>97.103165</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# - now let us ask a question which ONLY exits in the version 2 of the document\n",
    "search_query_v2 = \"Can i use Quantum computing?\"\n",
    "results_with_scores = vs.similarity_search_with_score(query=search_query_v2, filter=dict(page='v2'), k=1, fetch_k=3)\n",
    "results_with_scores = [[doc.page_content, doc.metadata, score] for doc, score in results_with_scores]\n",
    "results_with_scores.insert(0, ['Document', 'Meta-Data', 'Score'])\n",
    "display_table(results_with_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us continue to build our chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c61cab4",
   "metadata": {},
   "source": [
    "The prompt template is now altered to include both conversation memory as well as chat history as inputs along with the human input. Notice how the prompt also instructs Claude to not answer questions which it does not have the context for. This helps reduce hallucinations which is extremely important when creating end user facing applications which need to be factual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-create vector store and continue\n",
    "vs = FAISS.load_local('../faiss-index/langchain/', embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b25ac933-0cfb-4982-997b-f43365b7bf84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RAG_TEMPLATE = \"\"\"You are a helpful conversational assistant.\n",
    "\n",
    "If you are unsure about the answer OR the answer does not exist in the context, respond with\n",
    "\"Sorry but I do not understand your request. I am still learning so I appreciate your patience! 😊\n",
    "NEVER make up the answer.\n",
    "\n",
    "If the human greets you, simply introduce yourself.\n",
    "\n",
    "The context will be placed in <context></context> XML tags.\n",
    "\n",
    "<context>{context}</context>\n",
    "\n",
    "Do not include any xml tags in your response.\n",
    "\n",
    "{history}\n",
    "\n",
    "Human: {input}\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate.from_template(RAG_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85f5867",
   "metadata": {},
   "source": [
    "The new `ConversationWithRetrieval` class now includes a `get_context` function which searches our vector database based on the human input and combines it into the base prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4c905e61-fc05-400d-9efb-5b42b151b5a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConversationWithRetrieval:\n",
    "    def __init__(self, client, vector_store: FAISS=None, model_id: str=\"anthropic.claude-instant-v1\") -> None:\n",
    "        \"\"\"instantiates a new rag based conversation\n",
    "\n",
    "        Args:\n",
    "            vector_store (FAISS, optional): pre-populated vector store for searching context. Defaults to None.\n",
    "            model_id (str, optional): which bedrock model to use for the conversational agent. Defaults to \"anthropic.claude-instant-v1\".\n",
    "        \"\"\"\n",
    "\n",
    "        # store vector store\n",
    "        self.vector_store = vector_store\n",
    "\n",
    "        # instantiate memory\n",
    "        self.memory = ConversationBufferMemory(human_prefix=\"Human\", ai_prefix=\"Assistant\")\n",
    "\n",
    "        # instantiate LLM connection\n",
    "        self.llm = BedrockLLM(\n",
    "            client=client,\n",
    "            model_id=model_id,\n",
    "            model_kwargs={\n",
    "                \"max_tokens_to_sample\": 500,\n",
    "                \"temperature\": 0.0,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def ai_respond(self, user_input: str=None):\n",
    "        \"\"\"responds to the user input in the conversation with context used\n",
    "\n",
    "        Args:\n",
    "            user_input (str, optional): user input. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            ai_output (str): response from AI chatbot\n",
    "            search_results (list): context used in the completion\n",
    "        \"\"\"\n",
    "\n",
    "        # format the prompt with chat history and user input\n",
    "        context_string, search_results = self.get_context(user_input)\n",
    "        history = self.memory.load_memory_variables({})['history']\n",
    "        llm_input = PROMPT.format(history=history, input=user_input, context=context_string)\n",
    "\n",
    "        # respond to the user with the LLM\n",
    "        ai_output = self.llm(llm_input).strip()\n",
    "\n",
    "        # store the input and output\n",
    "        self.memory.chat_memory.add_user_message(user_input)\n",
    "        self.memory.chat_memory.add_ai_message(ai_output.strip())\n",
    "\n",
    "        return ai_output, search_results\n",
    "\n",
    "    def get_context(self, user_input, k=5):\n",
    "        \"\"\"returns context used in the completion\n",
    "\n",
    "        Args:\n",
    "            user_input (str): user input as a string\n",
    "            k (int, optional): number of results to return. Defaults to 5.\n",
    "\n",
    "        Returns:\n",
    "            context_string (str): context used in the completion as a string\n",
    "            search_results (list): context used in the completion as a list of Document objects\n",
    "        \"\"\"\n",
    "        search_results = self.vector_store.similarity_search(\n",
    "            user_input, k=k\n",
    "        )\n",
    "        context_string = '\\n\\n'.join([f'Document {ind+1}: ' + i.page_content for ind, i in enumerate(search_results)])\n",
    "        return context_string, search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd0bbee",
   "metadata": {},
   "source": [
    "Now the model can answer some specific domain questions based on our document database!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c282d088-aa3c-493e-b1f4-0d47c27ffed4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = ConversationWithRetrieval(boto3_bedrock, vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "faaaad87-48fe-495f-98d9-4c7cd95ad5b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "SageMaker Clarify allows you to detect various types of biases in ML models, including disparities in performance across demographic groups. Some common metrics it supports measuring include differences in error rates, precision and recall between groups. Checking for these types of imbalances is an important part of ensuring your model performs fairly for all users. I'd recommend using SageMaker Clarify to analyze your trained model and look for any unbalanced treatment or outcomes across important attributes like gender or ethnicity. Identifying potential issues is the first step to mitigating unfairness."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output, context = chat.ai_respond('How can I check for imbalances in my model?')\n",
    "display(Markdown(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7e995518-35e8-4106-b454-fc443c68f1f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "** AI Assistant Answer: ** \n",
       "SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor). For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups. For example, start by comparing the error rates (how likely a model's prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "** Relevant Documentation: ** \n",
       "[Document(page_content='What is Amazon SageMaker Autopilot?,\" SageMaker Autopilot is the industry’s first automated machine learning capability that gives you complete control and visibility into your ML models. SageMaker Autopilot automatically inspects raw data, applies feature processors, picks the best set of algorithms, trains and tunes multiple models, tracks their performance, and then ranks the models based on performance, all with just a few clicks. The result is the best-performing model that you can deploy at a fraction of the time normally required to train the model. You get full visibility into how the model was created and what’s in it, and SageMaker Autopilot integrates with SageMaker Studio. You can explore up to 50 different models generated by SageMaker Autopilot inside SageMaker Studio so it’s easy to pick the best model for your use case. SageMaker Autopilot can be used by people without ML experience to easily produce a model, or it can be used by experienced developers to quickly develop a baseline model on which teams can further iterate.\"', metadata={}), Document(page_content='What is Amazon SageMaker Studio Lab?,\" SageMaker Studio Lab is a free ML development environment that provides the compute, storage (up to 15 GB), and security—all at no cost—for anyone to learn and experiment with ML. All you need to get started is a valid email ID; you don’t need to configure infrastructure or manage identity and access or even sign up for an AWS account. SageMaker Studio Lab accelerates model building through GitHub integration, and it comes preconfigured with the most popular ML tools, frameworks, and libraries to get you started immediately. SageMaker Studio Lab automatically saves your work so you don’t need to restart between sessions. It’s as easy as closing your laptop and coming back later.\"', metadata={}), Document(page_content='What kind of bias does SageMaker Clarify detect?,\" Measuring bias in ML models is a first step to mitigating bias. Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You must choose bias notions and metrics that are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor). For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups. For example, start by comparing the error rates (how likely a model\\'s prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example).\"', metadata={}), Document(page_content='How can I reproduce a feature from a given moment in time?, SageMaker Feature Store maintains time stamps for all features at every instance of time. This helps you retrieve features at any period of time for business or compliance requirements. You can easily explain model features and their values from when they were first created to the present time by reproducing the model from a given moment in time.\\nWhat are offline features?,\" Offline features are used for training because you need access to very large volumes over a long period of time. These features are served from a high-throughput, high-bandwidth repository.\"\\nWhat are online features?, Online features are used in applications required to make real-time predictions. Online features are served from a high-throughput repository with single-digit millisecond latency for fast predictions.', metadata={}), Document(page_content='Why should I use SageMaker for shadow testing?,\" SageMaker simplifies the process of setting up and monitoring shadow variants so you can evaluate the performance of the new ML model on live production traffic. SageMaker eliminates the need for you to orchestrate infrastructure for shadow testing. It lets you control testing parameters such as the percentage of traffic mirrored to the shadow variant and the duration of the test. As a result, you can start small and increase the inference requests to the new model after you gain confidence in model performance. SageMaker creates a live dashboard displaying performance differences across key metrics, so you can easily compare model performance to evaluate how the new model differs from the production model.\"', metadata={})]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output, context = chat.ai_respond('What kind does it detect?')\n",
    "display(Markdown(f'** AI Assistant Answer: ** \\n{output}'))\n",
    "display(Markdown(f'\\n\\n** Relevant Documentation: ** \\n{context}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c22f06",
   "metadata": {},
   "source": [
    "--- \n",
    "## Using LangChain for Orchestration of RAG\n",
    "\n",
    "Beyond the primitive classes for prompt handling and conversational memory management, LangChain also provides a framework for [orchestrating RAG flows](https://python.langchain.com/docs/expression_language/cookbook/retrieval) with what purpose built \"chains\". In this section, we will see how to be a retrieval chain with LangChain which is more comprehensive and robust than the original retrieval system we built above.\n",
    "\n",
    "The workflow we used above follows the following process...\n",
    "\n",
    "1. User input is received.\n",
    "2. User input is queried against the vector database to retrieve relevant documents.\n",
    "3. Relevant documents and chat memory are inserted into a new prompt to respond to the user input.\n",
    "4. Return to step 1.\n",
    "\n",
    "However, more complex methods of interacting with the user input can generate more accurate results in RAG architectures. One of the popular mechanisms which can increase accuracy of these retrieval systems is utilizing more than one call to an LLM in order to reformat the user input for more effective search to your vector database. A better workflow is described below compared to the one we already built...\n",
    "\n",
    "1. User input is received.\n",
    "2. An LLM is used to reword the user input to be a better search query for the vector database based on the chat history and other instructions. This could include things like condensing, rewording, addition of chat context, or stylistic changes.\n",
    "3. Reformatted user input is queried against the vector database to retrieve relevant documents.\n",
    "4. The reformatted user input and relevant documents are inserted into a new prompt in order to answer the user question.\n",
    "5. Return to step 1.\n",
    "\n",
    "Let's now build out this second workflow using LangChain below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8611a13a",
   "metadata": {},
   "source": [
    "First we need to make a prompt which will reformat the user input to be more compatible for searching of the vector database. The way we do this is by providing the chat history as well as the some basic instructions to Claude and asking it to condense the input into a single output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e314e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "condense_prompt = PromptTemplate.from_template(\"\"\"\\\n",
    "<chat-history>\n",
    "{chat_history}\n",
    "</chat-history>\n",
    "\n",
    "<follow-up-message>\n",
    "{question}\n",
    "<follow-up-message>\n",
    "\n",
    "Human: Given the conversation above (between Human and Assistant) and the follow up message from Human, \\\n",
    "rewrite the follow up message to be a standalone question that captures all relevant context \\\n",
    "from the conversation. Answer only with the new question and nothing else.\n",
    "\n",
    "Assistant: Standalone Question:\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54d7099",
   "metadata": {},
   "source": [
    "The next prompt we need is the prompt which will answer the user's question based on the retrieved information. In this case, we provide specific instructions about how to answer the question as well as provide the context retrieved from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "55d854a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "respond_prompt = PromptTemplate.from_template(\"\"\"\\\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Human: Given the context above, answer the question inside the <q></q> XML tags.\n",
    "\n",
    "<q>{question}</q>\n",
    "\n",
    "If the answer is not in the context say \"Sorry, I don't know as the answer was not found in the context\". Do not use any XML tags in the answer.\n",
    "\n",
    "Assistant:\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7f9fbf",
   "metadata": {},
   "source": [
    "Now that we have our prompts set up, let's set up the conversational memory buffer just like we did earlier in the notebook. Notice how we inject an example human and assistant message in order to help guide our AI assistant on what its job is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "920aefbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = BedrockLLM(\n",
    "    client=boto3_bedrock,\n",
    "    model_id=\"anthropic.claude-instant-v1\",\n",
    "    model_kwargs={\"max_tokens_to_sample\": 500, \"temperature\": 0.9}\n",
    ")\n",
    "memory_chain = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    human_prefix=\"Human\",\n",
    "    ai_prefix=\"Assistant\"\n",
    ")\n",
    "memory_chain.chat_memory.add_user_message(\n",
    "    'Hello, what are you able to do?'\n",
    ")\n",
    "memory_chain.chat_memory.add_ai_message(\n",
    "    'Hi! I am a help chat assistant which can answer questions about Amazon SageMaker.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b223df37",
   "metadata": {},
   "source": [
    "Lastly, we will used the `ConversationalRetrievalChain` from LangChain to orchestrate this whole system. If you would like to see some more logs about what is happening in the orchestration and not just the final output, make sure to change the `verbose` argument to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c5da087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, # this is our claude model\n",
    "    retriever=vs.as_retriever(), # this is our FAISS vector database\n",
    "    memory=memory_chain, # this is the conversational memory storage class\n",
    "    condense_question_prompt=condense_prompt, # this is the prompt for condensing user inputs\n",
    "    verbose=False, # change this to True in order to see the logs working in the background\n",
    ")\n",
    "qa.combine_docs_chain.llm_chain.prompt = respond_prompt # this is the prompt in order to respond to condensed questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf46fba3",
   "metadata": {},
   "source": [
    "Let's go ahead and generate some responses from our RAG solution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8d10f20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " You can check for imbalances in your models trained using Amazon SageMaker by using Amazon SageMaker Clarify. SageMaker Clarify automatically detects concept drift and provides detailed alerts that help identify the source of the problem. All models trained in SageMaker automatically emit key metrics that can be collected and viewed in SageMaker Studio. From inside SageMaker Studio, you can configure data to be collected, how to view it, and when to receive alerts. This allows you to measure whether the performance of the model differs across groups and check for imbalances."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = qa.run({'question': 'How can I check for imbalances in my model?'})\n",
    "display(Markdown(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7af21687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " SageMaker Clarify can detect the following types of imbalances in a model:\n",
       "\n",
       "- Performance differences across demographic groups, such as higher error rates for one group.\n",
       "\n",
       "- Differences in precision or recall across groups. \n",
       "\n",
       "- Underrepresentation of certain groups in the training data. \n",
       "\n",
       "- Differences in the label distribution across demographic groups in the training data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = qa.run({'question': 'What kind does it detect?' })\n",
    "display(Markdown(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "61caec8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Checking for imbalances with Amazon SageMaker Clarify can help explain potential issues or biases in a model's performance across different demographic groups by measuring whether the performance of the model differs across groups. SageMaker Clarify supports computing different bias metrics for the trained model, such as comparing the error rates or precision and recall across groups, to identify if a model is performing differently for certain groups which could indicate unfairness or bias. This helps explain where any potential issues may lie in how a model treats different demographic groups."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = qa.run({'question': 'How does this improve model explainability?' })\n",
    "display(Markdown(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0502732a",
   "metadata": {},
   "source": [
    "--- \n",
    "## Using LlamaIndex for Orchestration of RAG\n",
    "\n",
    "Another popular open source framework for orchestrating RAG is [LlamaIndex](https://gpt-index.readthedocs.io/en/latest/index.html). Let's take a look below at how to use our SageMaker FAQ vector index to have a conversational RAG application with LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dcc96189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: llama-index-llms-langchain in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (0.4.1)\n",
      "Requirement already satisfied: langchain>=0.1.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-llms-langchain) (0.3.1)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-llms-langchain) (0.11.8)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain>=0.1.3->llama-index-llms-langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain>=0.1.3->llama-index-llms-langchain) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain>=0.1.3->llama-index-llms-langchain) (3.10.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain>=0.1.3->llama-index-llms-langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.6 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain>=0.1.3->llama-index-llms-langchain) (0.3.7)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain>=0.1.3->llama-index-llms-langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain>=0.1.3->llama-index-llms-langchain) (0.1.129)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain>=0.1.3->llama-index-llms-langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain>=0.1.3->llama-index-llms-langchain) (2.9.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain>=0.1.3->llama-index-llms-langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain>=0.1.3->llama-index-llms-langchain) (8.3.0)\n",
      "Requirement already satisfied: dataclasses-json in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (2024.6.1)\n",
      "Requirement already satisfied: httpx in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (3.2.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (3.9.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (9.4.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.3->llama-index-llms-langchain) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.3->llama-index-llms-langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.3->llama-index-llms-langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.3->llama-index-llms-langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.3->llama-index-llms-langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.3->llama-index-llms-langchain) (1.11.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.6->langchain>=0.1.3->llama-index-llms-langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.6->langchain>=0.1.3->llama-index-llms-langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langsmith<0.2.0,>=0.1.17->langchain>=0.1.3->llama-index-llms-langchain) (3.10.7)\n",
      "Requirement already satisfied: anyio in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (4.4.0)\n",
      "Requirement already satisfied: certifi in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (3.7)\n",
      "Requirement already satisfied: sniffio in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (0.14.0)\n",
      "Requirement already satisfied: click in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (2024.7.24)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.1.3->llama-index-llms-langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.1.3->llama-index-llms-langchain) (2.23.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain>=0.1.3->llama-index-llms-langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain>=0.1.3->llama-index-llms-langchain) (1.26.18)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from SQLAlchemy<3,>=1.4->langchain>=0.1.3->llama-index-llms-langchain) (3.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (3.22.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain>=0.1.3->llama-index-llms-langchain) (3.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-langchain) (1.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: llama-index-embeddings-langchain in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (0.2.1)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-embeddings-langchain) (0.11.8)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (2024.6.1)\n",
      "Requirement already satisfied: httpx in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (3.2.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (9.4.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (2.9.1)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (8.3.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (1.11.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (4.0.3)\n",
      "Requirement already satisfied: click in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (2024.7.24)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (2.23.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (3.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (3.22.0)\n",
      "Requirement already satisfied: anyio in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (0.14.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (23.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-langchain) (1.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: llama-index-vector-stores-faiss in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (0.2.1)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-vector-stores-faiss) (0.11.8)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (2024.6.1)\n",
      "Requirement already satisfied: httpx in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (3.2.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (9.4.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (2.9.1)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (8.3.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (1.11.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (4.0.3)\n",
      "Requirement already satisfied: click in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (2024.7.24)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (2.23.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (3.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (3.22.0)\n",
      "Requirement already satisfied: anyio in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (0.14.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (23.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-faiss) (1.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-llms-langchain\n",
    "%pip install llama-index-embeddings-langchain\n",
    "%pip install llama-index-vector-stores-faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6c059f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from langchain.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain_aws.llms import BedrockLLM\n",
    "\n",
    "from llama_index.core import Settings, StorageContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69279fe5",
   "metadata": {},
   "source": [
    "First we need to set up the system setting to define the embedding model and LLM. Again, we will be using titan and claude respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2677f2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' service_context = StorageContext.from_defaults(\\n    llm=llm, embed_model=embed_model, chunk_size=512\\n)  '"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model = BedrockEmbeddings(client=boto3_bedrock, model_id=\"amazon.titan-embed-text-v1\")\n",
    "llm = BedrockLLM(\n",
    "    client=boto3_bedrock,\n",
    "    model_id=\"anthropic.claude-instant-v1\",\n",
    "    model_kwargs={\n",
    "        \"max_tokens_to_sample\": 500,\n",
    "        \"temperature\": 0.9,\n",
    "    },\n",
    ")\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512\n",
    "\n",
    "\"\"\" service_context = StorageContext.from_defaults(\n",
    "    llm=llm, embed_model=embed_model, chunk_size=512\n",
    ")  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d5083",
   "metadata": {},
   "source": [
    "The next step would be to create a FAISS index from our document base. In this lab, this is already done for you and stored in the [faiss-index/llama-index/](../faiss-index/llama-index/) folder.\n",
    "\n",
    "If you are interested in how this was accomplished, follow [this tutorial](https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/FaissIndexDemo.html) from LlamIndex. The code below is the basics of how this was accomplished as well.\n",
    "\n",
    "```python\n",
    "# faiss_index = faiss.IndexFlatL2(1536)\n",
    "# vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "# documents = SimpleDirectoryReader(\"./../data/sagemaker\").load_data()\n",
    "# vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, storage_context=storage_context, service_context=service_context\n",
    "# )\n",
    "# index.storage_context.persist('../faiss-index/llama-index/')\n",
    "```\n",
    "\n",
    "Once the index is created, we can load the persistent files to a `FaissVectorStore` object and create a `query_engine` from the vector index. To learn more about indicies in LlamaIndex, read more [here](https://gpt-index.readthedocs.io/en/latest/understanding/indexing/indexing.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bf66c68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import load_index_from_storage, StorageContext\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "\n",
    "vector_store = FaissVectorStore.from_persist_path(\"../faiss-index/llama-index/vector_store.json\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store, persist_dir=\"../faiss-index/llama-index\"\n",
    ")\n",
    "index = load_index_from_storage(storage_context=storage_context) #, service_context = service_context)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92161d17",
   "metadata": {},
   "source": [
    "Now let's set up a retrieval based chat application similar to LangChain. We will use the same condensing question strategy as before and can reuse the same prompt to condense the question for vector searching. Notice how we include some custom chat history to inject context into the prompt for the model to understand what we are asking questions about. The resulting `chat_engine` object is now fully ready to chat about our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "942c2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts  import PromptTemplate\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core.chat_engine.condense_question import CondenseQuestionChatEngine\n",
    "\n",
    "custom_prompt = PromptTemplate(\"\"\"\\\n",
    "<chat-history>\n",
    "{chat_history}\n",
    "</chat-history>\n",
    "\n",
    "<follow-up-message>\n",
    "{question}\n",
    "<follow-up-message>\n",
    "\n",
    "Human: Given the conversation above (between Human and Assistant) and the follow up message from Human, \\\n",
    "rewrite the message to be a standalone question that captures all relevant context \\\n",
    "from the conversation. Answer only with the new question and nothing else.\n",
    "\n",
    "Assistant: Standalone Question:\"\"\")\n",
    "\n",
    "custom_chat_history = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content='Hello assistant, I have some questions about using Amazon SageMaker today.'\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=MessageRole.ASSISTANT,\n",
    "        content='Okay, sounds good.'\n",
    "    )\n",
    "]\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    condense_question_prompt=custom_prompt,\n",
    "    chat_history=custom_chat_history,\n",
    "    #service_context=service_context,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c59d3ba",
   "metadata": {},
   "source": [
    "Let's go ahead and ask our first question. Notice that the verbose `chat_engine` will print out the condensed question as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c61a330c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying with:  While exploring Amazon SageMaker, how can I check for imbalances in the models I'm working with?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       " Based on the context provided, to check for imbalances in models you are working with in Amazon SageMaker, you can use Amazon SageMaker Clarify. Specifically:\n",
       "\n",
       "- SageMaker Clarify helps improve model transparency by detecting statistical bias across the entire ML workflow. It checks for imbalances during data preparation, after training, and ongoing over time. \n",
       "\n",
       "- It includes tools to help explain ML models and their predictions. Findings can be shared through explainability reports.\n",
       "\n",
       "- It detects different types of bias like imbalances in the training data and differences in model performance across groups. It provides metrics to measure issues like differences in error rates or precision and recall across groups.\n",
       "\n",
       "- When used with SageMaker Experiments, it provides a feature importance graph to help understand a model's decision making process and determine if any inputs have undue influence. \n",
       "\n",
       "- It also makes explanations for individual predictions available via an API.\n",
       "\n",
       "So in summary, to check for imbalances in your SageMaker models, you can leverage the capabilities of Amazon SageMaker Clarify."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = chat_engine.chat(\"How can I check for imbalances in my model?\")\n",
    "output = str(response)\n",
    "display(Markdown(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd137da",
   "metadata": {},
   "source": [
    "Now follow up questions can be asked with conversational context in mind!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "30d2d125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying with:  How does using Amazon SageMaker Clarify to detect statistical bias and imbalance across the ML workflow, including during data preparation, training, and ongoing evaluation over time, help improve the explainability of models created with Amazon SageMaker?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       " Using Amazon SageMaker Clarify to detect statistical bias and imbalance across the ML workflow can help improve model explainability in several ways:\n",
       "\n",
       "1. By checking for imbalances in the training data during data preparation, SageMaker Clarify can identify any underrepresentation or differences in label distribution across groups in the data. This provides important context about the representativeness and quality of the training data.\n",
       "\n",
       "2. After a model has been trained, SageMaker Clarify can measure whether and how much the model's performance differs across groups. This helps evaluate if the trained model exhibits any statistical bias or imbalance. \n",
       "\n",
       "3. During ongoing evaluation over time with Model Monitor, SageMaker Clarify can detect if a model's predictions become biased or imbalanced after deployment, such as from data or distribution shifts. \n",
       "\n",
       "4. The findings from SageMaker Clarify's bias and imbalance checks can be compiled into explainability reports. This provides transparency into any issues regarding fairness, representativeness or statistical biases in the model.\n",
       "\n",
       "5. The detection of statistical biases and imbalances helps determine if certain model inputs have an undue influence on predictions for particular groups. This in turn improves understanding of a model's decision making process.\n",
       "\n",
       "So in summary, SageMaker Clarify's capabilities for detecting biases across the ML workflow provide important context for evaluating a model's performance, representativeness and fairness. This context directly helps improve the explainability of models created with SageMaker."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = chat_engine.chat(\"How does this improve model explainability?\")\n",
    "output = str(response)\n",
    "display(Markdown(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48a0e8-147d-4525-a6b2-68a09af1b2c4",
   "metadata": {},
   "source": [
    "---\n",
    "## Next steps\n",
    "\n",
    "Now that we have a working RAG application with vector search retrieval, we will explore a new type of retrieval. In the next notebook we will see how to use LLM agents to automatically retrieve information from APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
